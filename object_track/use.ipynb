{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd yolov5\n",
    "#!python train.py --img 640 --batch-size -1 --epochs 500 --data ../garbage.yaml --weights yolov5s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home1/wckdata/garbage_detect/yolov5\n"
     ]
    }
   ],
   "source": [
    "# Â∞Üresults.csvÂèòÊàêÂõæÁâáÁöÑÊ†ºÂºè\n",
    "%cd yolov5\n",
    "from utils.plots import plot_results\n",
    "plot_results('runs/train/exp/results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/home1/wckdata/garbage_detect/yolov5/runs/train/exp/results.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# È™åËØÅval‰∏≠ÁöÑÂõæÁâá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd yolov5\n",
    "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.25 --source ../datasets/DLLG_YOLO/val/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from IPython.display import display, Image\n",
    "\n",
    "directory_path=\"/home1/wckdata/garbage_detect/yolov5/runs/detect/exp/\"\n",
    "file_count=len([name for name in os.listdir(directory_path)])\n",
    "print(f'Êñá‰ª∂Â§π \"{directory_path}\" ‰∏≠ÁöÑÊñá‰ª∂Êï∞ÈáèÊòØ: {file_count}')\n",
    "\n",
    "#display inference on some test images\n",
    "images = glob.glob('/home1/wckdata/garbage_detect/yolov5/runs/detect/exp/*.jpg')\n",
    "\n",
    "for imageName in images[:3]: #assuming JPG\n",
    "    display(Image(filename=imageName, width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ËßÜÈ¢ëÊäΩÂ∏ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂØºÂÖ•ÊâÄÊúâÂøÖË¶ÅÁöÑÂ∫ì\n",
    "import cv2\n",
    "import os\n",
    "# ‰ªéÊåáÂÆöÁöÑË∑ØÂæÑËØªÂèñËßÜÈ¢ë\n",
    "cam = cv2.VideoCapture(\"./test_video/785124.mp4\")\n",
    "\n",
    "if not os.path.exists(\"./test_video/img\"):\n",
    "     os.mkdir(\"./test_video/img\")\n",
    "\n",
    "  \n",
    "# frame\n",
    "currentframe = 0\n",
    "  \n",
    "while 1:\n",
    "    ret, frame = cam.read()\n",
    "    if ret:\n",
    "        name = \"./test_video/img/\" + str(currentframe)+\".jpg\"\n",
    "        print(f\"create {name}\")\n",
    "        cv2.imwrite(name, frame)\n",
    "        currentframe += 100\n",
    "    else:\n",
    "        break\n",
    "  \n",
    "# ‰∏ÄÊó¶ÂÆåÊàêÈáäÊîæÊâÄÊúâÁöÑÁ©∫Èó¥ÂíåÁ™óÂè£\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelÂ§ÑÁêÜ‰∏ÄÂº†ÂõæÁâá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home1/wckdata/garbage_detect/yolov5\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn # cudaÊ®°Âùó\n",
    "import time\n",
    "\n",
    "ROOT = os.path.join(\"/home1/wckdata/garbage_detect/\", \"yolov5\")\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "print(str(ROOT))\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from yolov5.utils.torch_utils import select_device\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.general import (\n",
    "    check_img_size, non_max_suppression, apply_classifier, scale_boxes,\n",
    "    xyxy2xywh, strip_optimizer, set_logging, Profile, LOGGER\n",
    "    )\n",
    "from yolov5.utils.augmentations import letterbox\n",
    "from ultralytics.utils.plotting import Annotator, colors, save_one_box\n",
    "from yolov5.models.experimental import attempt_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-7-4 Python-3.8.19 torch-1.11.0+cu113 CUDA:0 (NVIDIA A40-12Q, 12099MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "device = select_device('0')\n",
    "model = attempt_load('./best_train/20240709best.pt', device=device)\n",
    "imgsz = check_img_size(640, s=model.stride.max()) # È™åËØÅÂõæÁâáÂ§ßÂ∞è\n",
    "model.half()\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_image = cv2.imread(\"/home1/wckdata/garbage_detect/test_video/img/17500.jpg\") #(h,w,bgr)\n",
    "color_image = np.asanyarray(read_image)\n",
    "# print(color_image.shape) #(480, 640, 3)\n",
    "imgs = [None]\n",
    "imgs[0] = color_image\n",
    "im0s = imgs.copy()\n",
    "img = [letterbox(x, new_shape=640)[0] for x in im0s]  # img: ËøõË°åresize + pad‰πãÂêéÁöÑÂõæÁâá listÊ†ºÂºè\n",
    "img = np.stack(img, 0)  #Ê≤øÁùÄ0dimËøõË°åÂ†ÜÂè† Ê≠§Êó∂shapeÂ∑≤ÁªèÊòØ[batch_size, h, w, channel]\n",
    "# print(img.shape) # (1, 480, 640, 3)\n",
    "img = img[:, :, :, ::-1].transpose(0, 3, 1, 2)  # BGR to RGB, to 3x416x416, uint8 to float32\n",
    "# print(img.shape) # (1, 3, 480, 640)\n",
    "img = np.ascontiguousarray(img, dtype=np.float16 if True else np.float32)\n",
    "                # ascontiguousarrayÂáΩÊï∞Â∞Ü‰∏Ä‰∏™ÂÜÖÂ≠ò‰∏çËøûÁª≠Â≠òÂÇ®ÁöÑÊï∞ÁªÑËΩ¨Êç¢‰∏∫ÂÜÖÂ≠òËøûÁª≠Â≠òÂÇ®ÁöÑÊï∞ÁªÑÔºå‰ΩøÂæóËøêË°åÈÄüÂ∫¶Êõ¥Âø´„ÄÇ\n",
    "img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "# print(img.shape) # (1, 3, 480, 640)\n",
    "img = torch.from_numpy(img).to(device) #Â∞ÜnumpyËΩ¨‰∏∫pytorchÁöÑtensor,Âπ∂ËΩ¨ÁßªÂà∞ËøêÁÆóËÆæÂ§á‰∏äËÆ°ÁÆó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[8.57031e+00, 6.50781e+00, 1.50156e+01,  ..., 3.35999e-02, 4.18945e-01, 2.04346e-01],\n",
      "         [1.49688e+01, 8.68750e+00, 2.86719e+01,  ..., 2.45209e-02, 2.97363e-01, 3.01758e-01],\n",
      "         [1.80156e+01, 7.93750e+00, 3.38750e+01,  ..., 3.70483e-02, 2.18018e-01, 3.78906e-01],\n",
      "         ...,\n",
      "         [5.70000e+02, 4.55000e+02, 2.09250e+02,  ..., 7.60742e-01, 1.92261e-01, 4.30298e-02],\n",
      "         [5.91000e+02, 4.53250e+02, 1.65250e+02,  ..., 6.31836e-01, 2.65381e-01, 7.34253e-02],\n",
      "         [6.15000e+02, 4.54000e+02, 1.93750e+02,  ..., 4.43359e-01, 3.60596e-01, 1.75903e-01]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "pred = model(img)[0]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[3.46750e+02, 2.11750e+02, 4.23750e+02, 2.74750e+02, 4.29443e-01, 1.00000e+00],\n",
      "        [2.66500e+02, 1.92750e+02, 3.55500e+02, 2.59500e+02, 2.98340e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "pred = non_max_suppression(pred, 0.25, 0.45)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # Â≠ó‰ΩìÂ§ßÂ∞è\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\t#ÈöèÊú∫È¢úËâ≤\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)   #ÈÄöËøáÁ°ÆÂÆöÂØπËßíÁ∫ø‰ΩçÁΩÆÊù•ÁîªÁü©ÂΩ¢Ê°Ü\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thicknessÂ≠ó‰ΩìÁ≤óÁªÜ\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "                    #(ÂõæÁâáimg,‚ÄúÊñáÊú¨ÂÜÖÂÆπ‚Äù,(Â∑¶‰∏ãËßíÂùêÊ†á),Â≠ó‰Ωì,Â≠ó‰ΩìÂ§ßÂ∞è,(È¢úËâ≤)ÔºåÁ∫øÊù°Á≤óÁªÜÔºåÁ∫øÊù°Á±ªÂûã)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = model.names\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
    "label_img = \"\"\n",
    "for i, det in enumerate(pred):\n",
    "    im0 = im0s[i].copy()\n",
    "    if len(det):\n",
    "        det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "        for c in det[:, 5].unique():\n",
    "            n = (det[:, 5] == c).sum()  # detections per class\n",
    "            s = f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            c = int(cls)  # integer class\n",
    "            label = f\"{names[c]}\"\n",
    "            confidence = float(conf)\n",
    "            confidence_str = f\"{confidence:.2f}\"\n",
    "            label_img = f\"{label}:{confidence_str}\"\n",
    "            plot_one_box(xyxy, im0, label=label_img, color=colors[int(cls)], line_thickness=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imwrite(\"/home1/wckdata/garbage_detect/test_video/img/result.jpg\", im0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
